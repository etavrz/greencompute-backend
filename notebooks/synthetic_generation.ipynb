{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"amazon.titan-text-premier-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel\n",
    "\n",
    "from greencompute_backend.services.llm.svc import EMBEDDINGS_MODEL\n",
    "\n",
    "class CustomEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        return HuggingFaceEmbeddings(\n",
    "\t\t\tmodel_name=EMBEDDINGS_MODEL\n",
    "\t\t)\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_query(text)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_query(text)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_documents(texts)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        \"Custom HugginFace Embeddings Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AmazonTitan(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        chat_model = LMFormatEnforcer(self.load_model(), json_schema=schema.model_json_schema())\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel | None = None) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Bedrock Model\"\n",
    "\n",
    "# Replace these with real values\n",
    "custom_model = ChatBedrock(\n",
    "    model_id=\"amazon.titan-text-premier-v1:0\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "amazon_titan = AmazonTitan(model=custom_model)\n",
    "# print(amazon_titan.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "\n",
    "synthesizer = Synthesizer(\n",
    "    model=amazon_titan,\n",
    "    critic_model=amazon_titan,\n",
    "    embedder=CustomEmbeddingModel(),\n",
    "    async_mode=False\n",
    ")\n",
    "synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=['output.txt'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "\n",
    "synthesizer = Synthesizer()\n",
    "synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=['output.txt'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
